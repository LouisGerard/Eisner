{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependency Parsing with Eisner\n",
    "Par Louis GERARD & Tiago CORTINHAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "venv_root = '/var/lib/vz/data/g13005245/DEEP/'\n",
    "sys.path.append(venv_root + 'lib/python3.5/site-packages')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features 1\n",
    "\n",
    "We start building a model on these features :\n",
    "- Governor POS\n",
    "- Dependant POS\n",
    "- Distance between these words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data\n",
    "\n",
    "We read the conllu file to get aall the info we need. In order to make this reusable, we can add any feature to the final list by adding its index to `features_enabled`. Here we have :\n",
    "- the index, to identify our token\n",
    "- the part of speech, which we want to pass to our model\n",
    "- the governor index, to build pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>0</td>\n",
       "      <td>root</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ADV</td>\n",
       "      <td>5</td>\n",
       "      <td>advmod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>5</td>\n",
       "      <td>punct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>DET</td>\n",
       "      <td>4</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>5</td>\n",
       "      <td>nsubj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>VERB</td>\n",
       "      <td>0</td>\n",
       "      <td>root</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>DET</td>\n",
       "      <td>7</td>\n",
       "      <td>det</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>5</td>\n",
       "      <td>obj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>5</td>\n",
       "      <td>punct</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0      1  2       3\n",
       "0  0   ROOT  0    root\n",
       "1  1    ADV  5  advmod\n",
       "2  2  PUNCT  5   punct\n",
       "3  3    DET  4     det\n",
       "4  4   NOUN  5   nsubj\n",
       "5  5   VERB  0    root\n",
       "6  6    DET  7     det\n",
       "7  7   NOUN  5     obj\n",
       "8  8  PUNCT  5   punct"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_conllu(conllu_filename):\n",
    "    sentences = []\n",
    "\n",
    "    features = ['INDEX', 'FORM', 'LEMMA', 'POS', 'X1', 'MORPHO', 'GOV', 'LABEL', 'X2', 'X3']\n",
    "    features_enabled = [0, 3, 6, 7]\n",
    "    root = [0, 'ROOT', 0, 'root']\n",
    "\n",
    "    columns = []\n",
    "    for i, f in enumerate(features):\n",
    "        if i in features_enabled:\n",
    "            columns.append(f)\n",
    "\n",
    "    with open(conllu_filename, 'r', encoding='utf-8') as conllu_file:\n",
    "        sentence = []\n",
    "        for i in range(len(features_enabled)):\n",
    "            sentence.append([root[i]])\n",
    "        tokens = []\n",
    "        for line in conllu_file:\n",
    "            if line[0] == '\\n':\n",
    "                if len(sentence) > 1:\n",
    "                    sentences.append(sentence)\n",
    "                    sentence = []\n",
    "                    for i in range(len(features_enabled)):\n",
    "                        sentence.append([root[i]])\n",
    "            elif line[0] == '#':\n",
    "                pass\n",
    "            else:\n",
    "                tokens = line.split('\\t')\n",
    "                if '-' not in tokens[0]:\n",
    "                    for i, j in enumerate(features_enabled):\n",
    "                        sentence[i].append(tokens[j])\n",
    "    return sentences\n",
    "\n",
    "sentences = read_conllu(\"UD_French-GSD/fr_gsd-ud-train.conllu\")\n",
    "pd.DataFrame(sentences[3]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all the data we need, we can process it to build our dataset. \n",
    "\n",
    "We can't pass directly the part of speech tags to our model, so we need to encod it to an one hot vector. We can compute the distances with the indexes. \n",
    "\n",
    "We can still reuse this code by adapting `index_i`, `pos_i` and `governor_i` to the previous final result and process the added data in `create_example()`. \n",
    "\n",
    "The targets are a bit simplitic because all we need to compute it is the distance. A single neuron with linear activation function, a weight of 1 for the distance and weights of 0 for the POS can do the job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield np.array(sentence).T\n",
    "        \n",
    "pos_2_1hot = {\n",
    "    'ADJ': 1,\n",
    "    'ADP': 2,\n",
    "    'ADV': 3,\n",
    "    'AUX': 4,\n",
    "    'CCONJ': 5,\n",
    "    'DET': 6,\n",
    "    'INTJ': 7,\n",
    "    'NOUN': 8,\n",
    "    'NUM': 9,\n",
    "    'PART': 10,\n",
    "    'PRON': 11,\n",
    "    'PROPN': 12,\n",
    "    'PUNCT': 13,\n",
    "    'SCONJ': 14,\n",
    "    'SYM': 15,\n",
    "    'VERB': 16,\n",
    "    'X': 17,\n",
    "    'ROOT': 0\n",
    "}\n",
    "\n",
    "labels_2_1hot = {\n",
    "    'acl': 0,\n",
    "    'advcl': 1,\n",
    "    'advmod': 2,\n",
    "    'amod': 3,\n",
    "    'appos': 4,\n",
    "    'aux': 5,\n",
    "    'case': 6,\n",
    "    'cc': 7,\n",
    "    'ccomp': 8,\n",
    "    'clf': 9,\n",
    "    'compound': 10,\n",
    "    'conj': 11,\n",
    "    'cop': 12,\n",
    "    'csubj': 13,\n",
    "    'dep': 14,\n",
    "    'det': 15,\n",
    "    'discourse': 16,\n",
    "    'dislocated': 17,\n",
    "    'expl': 18,\n",
    "    'fixed': 19,\n",
    "    'flat': 20,\n",
    "    'goeswith': 21,\n",
    "    'iobj': 22,\n",
    "    'list': 23,\n",
    "    'mark': 24,\n",
    "    'nmod': 25,\n",
    "    'nsubj': 26,\n",
    "    'nummod': 27,\n",
    "    'obj': 28,\n",
    "    'obl': 29,\n",
    "    'orphan': 30,\n",
    "    'parataxis': 31,\n",
    "    'punct': 32,\n",
    "    'reparandum': 33,\n",
    "    'root': 34,\n",
    "    'vocative': 35,\n",
    "    'xcomp': 36\n",
    "}\n",
    "\n",
    "index_i = 0\n",
    "pos_i = 1\n",
    "governor_i = 2\n",
    "label_i = 3\n",
    "\n",
    "def create_example(w1, w2, positive=True):\n",
    "    dist = int(w2[index_i]) - int(w1[index_i])\n",
    "\n",
    "    pos1 = np.zeros(len(pos_2_1hot))\n",
    "    pos1[pos_2_1hot[w1[pos_i]]] = 1\n",
    "\n",
    "    pos2 = np.zeros(len(pos_2_1hot))\n",
    "    pos2[pos_2_1hot[w2[pos_i]]] = 1\n",
    "    \n",
    "    x = np.concatenate(([dist], pos1, pos2))\n",
    "    label = np.zeros(37)\n",
    "    l = w1[label_i].split(':', 1)[0]\n",
    "    label[labels_2_1hot[l]] = 1\n",
    "    if positive:\n",
    "        y = [dist > 0, dist <= 0]\n",
    "    else:\n",
    "        y = [0, 0]\n",
    "    return x, np.concatenate((y, label))\n",
    "\n",
    "def create_dataset(sentences):\n",
    "    x = []\n",
    "    y = []\n",
    "    for s in get_sentence(sentences):\n",
    "        for w1 in s:\n",
    "            if w1[index_i] == '0':\n",
    "                continue\n",
    "            w2 = s[int(w1[governor_i])]\n",
    "\n",
    "            x_token, y_token = create_example(w1, w2)\n",
    "\n",
    "            x.append(x_token)\n",
    "            y.append(y_token)\n",
    "\n",
    "    return np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((356638, 37), (356638, 39))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = create_dataset(sentences)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 39)                1482      \n",
      "=================================================================\n",
      "Total params: 1,482\n",
      "Trainable params: 1,482\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "# model.add(keras.layers.Dense(37, activation='sigmoid', input_dim=37))\n",
    "model.add(keras.layers.Dense(39, activation='sigmoid', input_dim=37))\n",
    "\n",
    "model.compile('sgd', 'binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 249646 samples, validate on 106992 samples\n",
      "Epoch 1/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0589 - acc: 0.9848 - val_loss: 0.0590 - val_acc: 0.9848\n",
      "Epoch 2/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0588 - acc: 0.9848 - val_loss: 0.0590 - val_acc: 0.9848\n",
      "Epoch 3/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0588 - acc: 0.9848 - val_loss: 0.0590 - val_acc: 0.9848\n",
      "Epoch 4/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0588 - acc: 0.9848 - val_loss: 0.0590 - val_acc: 0.9848\n",
      "Epoch 5/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0587 - acc: 0.9848 - val_loss: 0.0589 - val_acc: 0.9848\n",
      "Epoch 6/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0587 - acc: 0.9848 - val_loss: 0.0589 - val_acc: 0.9848\n",
      "Epoch 7/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0587 - acc: 0.9848 - val_loss: 0.0589 - val_acc: 0.9848\n",
      "Epoch 8/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0587 - acc: 0.9848 - val_loss: 0.0588 - val_acc: 0.9848\n",
      "Epoch 9/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0586 - acc: 0.9848 - val_loss: 0.0588 - val_acc: 0.9848\n",
      "Epoch 10/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0586 - acc: 0.9848 - val_loss: 0.0588 - val_acc: 0.9848\n",
      "Epoch 11/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0586 - acc: 0.9848 - val_loss: 0.0588 - val_acc: 0.9848\n",
      "Epoch 12/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0586 - acc: 0.9848 - val_loss: 0.0587 - val_acc: 0.9848\n",
      "Epoch 13/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0585 - acc: 0.9848 - val_loss: 0.0587 - val_acc: 0.9848\n",
      "Epoch 14/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0585 - acc: 0.9849 - val_loss: 0.0587 - val_acc: 0.9848\n",
      "Epoch 15/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0585 - acc: 0.9849 - val_loss: 0.0587 - val_acc: 0.9848\n",
      "Epoch 16/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0584 - acc: 0.9848 - val_loss: 0.0586 - val_acc: 0.9848\n",
      "Epoch 17/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0584 - acc: 0.9849 - val_loss: 0.0586 - val_acc: 0.9848\n",
      "Epoch 18/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0584 - acc: 0.9849 - val_loss: 0.0586 - val_acc: 0.9848\n",
      "Epoch 19/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0584 - acc: 0.9849 - val_loss: 0.0585 - val_acc: 0.9848\n",
      "Epoch 20/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0583 - acc: 0.9850 - val_loss: 0.0585 - val_acc: 0.9850\n",
      "Epoch 21/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0583 - acc: 0.9851 - val_loss: 0.0585 - val_acc: 0.9850\n",
      "Epoch 22/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0583 - acc: 0.9851 - val_loss: 0.0585 - val_acc: 0.9850\n",
      "Epoch 23/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0583 - acc: 0.9851 - val_loss: 0.0584 - val_acc: 0.9850\n",
      "Epoch 24/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0582 - acc: 0.9851 - val_loss: 0.0584 - val_acc: 0.9850\n",
      "Epoch 25/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0582 - acc: 0.9851 - val_loss: 0.0584 - val_acc: 0.9850\n",
      "Epoch 26/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0582 - acc: 0.9851 - val_loss: 0.0584 - val_acc: 0.9850\n",
      "Epoch 27/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0581 - acc: 0.9851 - val_loss: 0.0583 - val_acc: 0.9850\n",
      "Epoch 28/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0581 - acc: 0.9851 - val_loss: 0.0583 - val_acc: 0.9850\n",
      "Epoch 29/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0581 - acc: 0.9851 - val_loss: 0.0583 - val_acc: 0.9850\n",
      "Epoch 30/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0581 - acc: 0.9851 - val_loss: 0.0583 - val_acc: 0.9850\n",
      "Epoch 31/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0580 - acc: 0.9851 - val_loss: 0.0582 - val_acc: 0.9850\n",
      "Epoch 32/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0580 - acc: 0.9851 - val_loss: 0.0582 - val_acc: 0.9850\n",
      "Epoch 33/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0580 - acc: 0.9851 - val_loss: 0.0582 - val_acc: 0.9850\n",
      "Epoch 34/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0580 - acc: 0.9851 - val_loss: 0.0581 - val_acc: 0.9850\n",
      "Epoch 35/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0579 - acc: 0.9851 - val_loss: 0.0581 - val_acc: 0.9850\n",
      "Epoch 36/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0579 - acc: 0.9851 - val_loss: 0.0581 - val_acc: 0.9850\n",
      "Epoch 37/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0579 - acc: 0.9851 - val_loss: 0.0581 - val_acc: 0.9850\n",
      "Epoch 38/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0579 - acc: 0.9851 - val_loss: 0.0580 - val_acc: 0.9850\n",
      "Epoch 39/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0578 - acc: 0.9851 - val_loss: 0.0580 - val_acc: 0.9850\n",
      "Epoch 40/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0578 - acc: 0.9851 - val_loss: 0.0580 - val_acc: 0.9850\n",
      "Epoch 41/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0578 - acc: 0.9851 - val_loss: 0.0580 - val_acc: 0.9850\n",
      "Epoch 42/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0578 - acc: 0.9851 - val_loss: 0.0579 - val_acc: 0.9850\n",
      "Epoch 43/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0577 - acc: 0.9851 - val_loss: 0.0579 - val_acc: 0.9850\n",
      "Epoch 44/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0577 - acc: 0.9851 - val_loss: 0.0579 - val_acc: 0.9850\n",
      "Epoch 45/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0577 - acc: 0.9851 - val_loss: 0.0579 - val_acc: 0.9850\n",
      "Epoch 46/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0577 - acc: 0.9851 - val_loss: 0.0578 - val_acc: 0.9850\n",
      "Epoch 47/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0576 - acc: 0.9851 - val_loss: 0.0578 - val_acc: 0.9850\n",
      "Epoch 48/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0576 - acc: 0.9851 - val_loss: 0.0578 - val_acc: 0.9850\n",
      "Epoch 49/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0576 - acc: 0.9851 - val_loss: 0.0578 - val_acc: 0.9850\n",
      "Epoch 50/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0575 - acc: 0.9851 - val_loss: 0.0577 - val_acc: 0.9850\n",
      "Epoch 51/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0575 - acc: 0.9851 - val_loss: 0.0577 - val_acc: 0.9850\n",
      "Epoch 52/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0575 - acc: 0.9851 - val_loss: 0.0577 - val_acc: 0.9850\n",
      "Epoch 53/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0575 - acc: 0.9851 - val_loss: 0.0577 - val_acc: 0.9850\n",
      "Epoch 54/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0574 - acc: 0.9851 - val_loss: 0.0576 - val_acc: 0.9850\n",
      "Epoch 55/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0574 - acc: 0.9851 - val_loss: 0.0576 - val_acc: 0.9850\n",
      "Epoch 56/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0574 - acc: 0.9851 - val_loss: 0.0576 - val_acc: 0.9850\n",
      "Epoch 57/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0574 - acc: 0.9851 - val_loss: 0.0576 - val_acc: 0.9851\n",
      "Epoch 58/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0573 - acc: 0.9851 - val_loss: 0.0575 - val_acc: 0.9851\n",
      "Epoch 59/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0573 - acc: 0.9851 - val_loss: 0.0575 - val_acc: 0.9851\n",
      "Epoch 60/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0573 - acc: 0.9851 - val_loss: 0.0575 - val_acc: 0.9851\n",
      "Epoch 61/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0573 - acc: 0.9851 - val_loss: 0.0575 - val_acc: 0.9851\n",
      "Epoch 62/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0572 - acc: 0.9851 - val_loss: 0.0574 - val_acc: 0.9851\n",
      "Epoch 63/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0572 - acc: 0.9851 - val_loss: 0.0574 - val_acc: 0.9851\n",
      "Epoch 64/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0572 - acc: 0.9851 - val_loss: 0.0574 - val_acc: 0.9851\n",
      "Epoch 65/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0572 - acc: 0.9851 - val_loss: 0.0574 - val_acc: 0.9851\n",
      "Epoch 66/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0571 - acc: 0.9851 - val_loss: 0.0573 - val_acc: 0.9851\n",
      "Epoch 67/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0571 - acc: 0.9851 - val_loss: 0.0573 - val_acc: 0.9851\n",
      "Epoch 68/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0571 - acc: 0.9851 - val_loss: 0.0573 - val_acc: 0.9851\n",
      "Epoch 69/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0571 - acc: 0.9851 - val_loss: 0.0573 - val_acc: 0.9851\n",
      "Epoch 70/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0570 - acc: 0.9851 - val_loss: 0.0572 - val_acc: 0.9851\n",
      "Epoch 71/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0570 - acc: 0.9851 - val_loss: 0.0572 - val_acc: 0.9851\n",
      "Epoch 72/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0570 - acc: 0.9851 - val_loss: 0.0572 - val_acc: 0.9851\n",
      "Epoch 73/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0570 - acc: 0.9851 - val_loss: 0.0572 - val_acc: 0.9851\n",
      "Epoch 74/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0569 - acc: 0.9851 - val_loss: 0.0571 - val_acc: 0.9851\n",
      "Epoch 75/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0569 - acc: 0.9851 - val_loss: 0.0571 - val_acc: 0.9851\n",
      "Epoch 76/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0569 - acc: 0.9851 - val_loss: 0.0571 - val_acc: 0.9851\n",
      "Epoch 77/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0569 - acc: 0.9851 - val_loss: 0.0571 - val_acc: 0.9851\n",
      "Epoch 78/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0568 - acc: 0.9851 - val_loss: 0.0570 - val_acc: 0.9851\n",
      "Epoch 79/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0568 - acc: 0.9851 - val_loss: 0.0570 - val_acc: 0.9851\n",
      "Epoch 80/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0568 - acc: 0.9851 - val_loss: 0.0570 - val_acc: 0.9851\n",
      "Epoch 81/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0568 - acc: 0.9851 - val_loss: 0.0570 - val_acc: 0.9851\n",
      "Epoch 82/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0568 - acc: 0.9851 - val_loss: 0.0569 - val_acc: 0.9851\n",
      "Epoch 83/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0567 - acc: 0.9851 - val_loss: 0.0569 - val_acc: 0.9851\n",
      "Epoch 84/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0567 - acc: 0.9851 - val_loss: 0.0569 - val_acc: 0.9851\n",
      "Epoch 85/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0567 - acc: 0.9851 - val_loss: 0.0569 - val_acc: 0.9851\n",
      "Epoch 86/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0567 - acc: 0.9851 - val_loss: 0.0568 - val_acc: 0.9851\n",
      "Epoch 87/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0566 - acc: 0.9851 - val_loss: 0.0568 - val_acc: 0.9851\n",
      "Epoch 88/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0566 - acc: 0.9851 - val_loss: 0.0568 - val_acc: 0.9851\n",
      "Epoch 89/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0566 - acc: 0.9851 - val_loss: 0.0568 - val_acc: 0.9851\n",
      "Epoch 90/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0566 - acc: 0.9851 - val_loss: 0.0568 - val_acc: 0.9851\n",
      "Epoch 91/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0565 - acc: 0.9851 - val_loss: 0.0567 - val_acc: 0.9851\n",
      "Epoch 92/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0565 - acc: 0.9851 - val_loss: 0.0567 - val_acc: 0.9851\n",
      "Epoch 93/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0565 - acc: 0.9851 - val_loss: 0.0567 - val_acc: 0.9851\n",
      "Epoch 94/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0565 - acc: 0.9851 - val_loss: 0.0567 - val_acc: 0.9851\n",
      "Epoch 95/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0564 - acc: 0.9851 - val_loss: 0.0566 - val_acc: 0.9851\n",
      "Epoch 96/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0564 - acc: 0.9851 - val_loss: 0.0566 - val_acc: 0.9851\n",
      "Epoch 97/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0564 - acc: 0.9851 - val_loss: 0.0566 - val_acc: 0.9851\n",
      "Epoch 98/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0564 - acc: 0.9851 - val_loss: 0.0566 - val_acc: 0.9851\n",
      "Epoch 99/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0563 - acc: 0.9851 - val_loss: 0.0565 - val_acc: 0.9851\n",
      "Epoch 100/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0563 - acc: 0.9851 - val_loss: 0.0565 - val_acc: 0.9851\n",
      "Epoch 101/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0563 - acc: 0.9851 - val_loss: 0.0565 - val_acc: 0.9851\n",
      "Epoch 102/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0563 - acc: 0.9851 - val_loss: 0.0565 - val_acc: 0.9851\n",
      "Epoch 103/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0562 - acc: 0.9851 - val_loss: 0.0564 - val_acc: 0.9851\n",
      "Epoch 104/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0562 - acc: 0.9851 - val_loss: 0.0564 - val_acc: 0.9851\n",
      "Epoch 105/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0562 - acc: 0.9851 - val_loss: 0.0564 - val_acc: 0.9851\n",
      "Epoch 106/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0562 - acc: 0.9851 - val_loss: 0.0564 - val_acc: 0.9851\n",
      "Epoch 107/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0562 - acc: 0.9851 - val_loss: 0.0563 - val_acc: 0.9851\n",
      "Epoch 108/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0561 - acc: 0.9851 - val_loss: 0.0563 - val_acc: 0.9851\n",
      "Epoch 109/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0561 - acc: 0.9851 - val_loss: 0.0563 - val_acc: 0.9851\n",
      "Epoch 110/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0561 - acc: 0.9851 - val_loss: 0.0563 - val_acc: 0.9851\n",
      "Epoch 111/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0561 - acc: 0.9851 - val_loss: 0.0563 - val_acc: 0.9851\n",
      "Epoch 112/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0560 - acc: 0.9851 - val_loss: 0.0562 - val_acc: 0.9851\n",
      "Epoch 113/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0560 - acc: 0.9851 - val_loss: 0.0562 - val_acc: 0.9851\n",
      "Epoch 114/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0560 - acc: 0.9851 - val_loss: 0.0562 - val_acc: 0.9851\n",
      "Epoch 115/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0560 - acc: 0.9851 - val_loss: 0.0562 - val_acc: 0.9851\n",
      "Epoch 116/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0559 - acc: 0.9851 - val_loss: 0.0561 - val_acc: 0.9851\n",
      "Epoch 117/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0559 - acc: 0.9851 - val_loss: 0.0561 - val_acc: 0.9851\n",
      "Epoch 118/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0559 - acc: 0.9851 - val_loss: 0.0561 - val_acc: 0.9851\n",
      "Epoch 119/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0559 - acc: 0.9851 - val_loss: 0.0561 - val_acc: 0.9851\n",
      "Epoch 120/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0559 - acc: 0.9851 - val_loss: 0.0561 - val_acc: 0.9851\n",
      "Epoch 121/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0558 - acc: 0.9851 - val_loss: 0.0560 - val_acc: 0.9851\n",
      "Epoch 122/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0558 - acc: 0.9851 - val_loss: 0.0560 - val_acc: 0.9851\n",
      "Epoch 123/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0558 - acc: 0.9851 - val_loss: 0.0560 - val_acc: 0.9851\n",
      "Epoch 124/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0558 - acc: 0.9851 - val_loss: 0.0560 - val_acc: 0.9851\n",
      "Epoch 125/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0557 - acc: 0.9851 - val_loss: 0.0559 - val_acc: 0.9851\n",
      "Epoch 126/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0557 - acc: 0.9851 - val_loss: 0.0559 - val_acc: 0.9851\n",
      "Epoch 127/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0557 - acc: 0.9851 - val_loss: 0.0559 - val_acc: 0.9851\n",
      "Epoch 128/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0557 - acc: 0.9851 - val_loss: 0.0559 - val_acc: 0.9851\n",
      "Epoch 129/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0557 - acc: 0.9851 - val_loss: 0.0558 - val_acc: 0.9851\n",
      "Epoch 130/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0556 - acc: 0.9851 - val_loss: 0.0558 - val_acc: 0.9851\n",
      "Epoch 131/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0556 - acc: 0.9851 - val_loss: 0.0558 - val_acc: 0.9851\n",
      "Epoch 132/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0556 - acc: 0.9851 - val_loss: 0.0558 - val_acc: 0.9851\n",
      "Epoch 133/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0556 - acc: 0.9851 - val_loss: 0.0558 - val_acc: 0.9851\n",
      "Epoch 134/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0555 - acc: 0.9852 - val_loss: 0.0557 - val_acc: 0.9851\n",
      "Epoch 135/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0555 - acc: 0.9852 - val_loss: 0.0557 - val_acc: 0.9851\n",
      "Epoch 136/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0555 - acc: 0.9852 - val_loss: 0.0557 - val_acc: 0.9851\n",
      "Epoch 137/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0555 - acc: 0.9852 - val_loss: 0.0557 - val_acc: 0.9851\n",
      "Epoch 138/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0554 - acc: 0.9852 - val_loss: 0.0556 - val_acc: 0.9851\n",
      "Epoch 139/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0554 - acc: 0.9852 - val_loss: 0.0556 - val_acc: 0.9851\n",
      "Epoch 140/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0554 - acc: 0.9852 - val_loss: 0.0556 - val_acc: 0.9851\n",
      "Epoch 141/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0554 - acc: 0.9852 - val_loss: 0.0556 - val_acc: 0.9851\n",
      "Epoch 142/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0554 - acc: 0.9852 - val_loss: 0.0556 - val_acc: 0.9851\n",
      "Epoch 143/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0553 - acc: 0.9852 - val_loss: 0.0555 - val_acc: 0.9851\n",
      "Epoch 144/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0553 - acc: 0.9852 - val_loss: 0.0555 - val_acc: 0.9851\n",
      "Epoch 145/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0553 - acc: 0.9852 - val_loss: 0.0555 - val_acc: 0.9851\n",
      "Epoch 146/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0553 - acc: 0.9852 - val_loss: 0.0555 - val_acc: 0.9851\n",
      "Epoch 147/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0553 - acc: 0.9852 - val_loss: 0.0554 - val_acc: 0.9851\n",
      "Epoch 148/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0552 - acc: 0.9852 - val_loss: 0.0554 - val_acc: 0.9851\n",
      "Epoch 149/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0552 - acc: 0.9852 - val_loss: 0.0554 - val_acc: 0.9851\n",
      "Epoch 150/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0552 - acc: 0.9852 - val_loss: 0.0554 - val_acc: 0.9851\n",
      "Epoch 151/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0552 - acc: 0.9852 - val_loss: 0.0554 - val_acc: 0.9851\n",
      "Epoch 152/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0551 - acc: 0.9852 - val_loss: 0.0553 - val_acc: 0.9851\n",
      "Epoch 153/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0551 - acc: 0.9852 - val_loss: 0.0553 - val_acc: 0.9851\n",
      "Epoch 154/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0551 - acc: 0.9852 - val_loss: 0.0553 - val_acc: 0.9851\n",
      "Epoch 155/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0551 - acc: 0.9852 - val_loss: 0.0553 - val_acc: 0.9851\n",
      "Epoch 156/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0551 - acc: 0.9852 - val_loss: 0.0553 - val_acc: 0.9851\n",
      "Epoch 157/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0550 - acc: 0.9852 - val_loss: 0.0552 - val_acc: 0.9851\n",
      "Epoch 158/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0550 - acc: 0.9852 - val_loss: 0.0552 - val_acc: 0.9851\n",
      "Epoch 159/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0550 - acc: 0.9852 - val_loss: 0.0552 - val_acc: 0.9851\n",
      "Epoch 160/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0550 - acc: 0.9852 - val_loss: 0.0552 - val_acc: 0.9851\n",
      "Epoch 161/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0549 - acc: 0.9852 - val_loss: 0.0551 - val_acc: 0.9851\n",
      "Epoch 162/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0549 - acc: 0.9852 - val_loss: 0.0551 - val_acc: 0.9851\n",
      "Epoch 163/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0549 - acc: 0.9852 - val_loss: 0.0551 - val_acc: 0.9851\n",
      "Epoch 164/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0549 - acc: 0.9852 - val_loss: 0.0551 - val_acc: 0.9851\n",
      "Epoch 165/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0549 - acc: 0.9852 - val_loss: 0.0551 - val_acc: 0.9851\n",
      "Epoch 166/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0548 - acc: 0.9852 - val_loss: 0.0550 - val_acc: 0.9851\n",
      "Epoch 167/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0548 - acc: 0.9852 - val_loss: 0.0550 - val_acc: 0.9851\n",
      "Epoch 168/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0548 - acc: 0.9852 - val_loss: 0.0550 - val_acc: 0.9851\n",
      "Epoch 169/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0548 - acc: 0.9852 - val_loss: 0.0550 - val_acc: 0.9851\n",
      "Epoch 170/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0548 - acc: 0.9852 - val_loss: 0.0550 - val_acc: 0.9851\n",
      "Epoch 171/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0547 - acc: 0.9852 - val_loss: 0.0549 - val_acc: 0.9851\n",
      "Epoch 172/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0547 - acc: 0.9852 - val_loss: 0.0549 - val_acc: 0.9851\n",
      "Epoch 173/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0547 - acc: 0.9852 - val_loss: 0.0549 - val_acc: 0.9851\n",
      "Epoch 174/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0547 - acc: 0.9852 - val_loss: 0.0549 - val_acc: 0.9851\n",
      "Epoch 175/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0546 - acc: 0.9852 - val_loss: 0.0548 - val_acc: 0.9851\n",
      "Epoch 176/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0546 - acc: 0.9852 - val_loss: 0.0548 - val_acc: 0.9851\n",
      "Epoch 177/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0546 - acc: 0.9852 - val_loss: 0.0548 - val_acc: 0.9851\n",
      "Epoch 178/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0546 - acc: 0.9852 - val_loss: 0.0548 - val_acc: 0.9851\n",
      "Epoch 179/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0546 - acc: 0.9852 - val_loss: 0.0548 - val_acc: 0.9851\n",
      "Epoch 180/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0545 - acc: 0.9852 - val_loss: 0.0547 - val_acc: 0.9851\n",
      "Epoch 181/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0545 - acc: 0.9852 - val_loss: 0.0547 - val_acc: 0.9851\n",
      "Epoch 182/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0545 - acc: 0.9852 - val_loss: 0.0547 - val_acc: 0.9851\n",
      "Epoch 183/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0545 - acc: 0.9852 - val_loss: 0.0547 - val_acc: 0.9851\n",
      "Epoch 184/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0545 - acc: 0.9852 - val_loss: 0.0547 - val_acc: 0.9851\n",
      "Epoch 185/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0544 - acc: 0.9852 - val_loss: 0.0546 - val_acc: 0.9851\n",
      "Epoch 186/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0544 - acc: 0.9852 - val_loss: 0.0546 - val_acc: 0.9851\n",
      "Epoch 187/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0544 - acc: 0.9852 - val_loss: 0.0546 - val_acc: 0.9851\n",
      "Epoch 188/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0544 - acc: 0.9852 - val_loss: 0.0546 - val_acc: 0.9851\n",
      "Epoch 189/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0544 - acc: 0.9852 - val_loss: 0.0546 - val_acc: 0.9851\n",
      "Epoch 190/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0543 - acc: 0.9852 - val_loss: 0.0545 - val_acc: 0.9851\n",
      "Epoch 191/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0543 - acc: 0.9852 - val_loss: 0.0545 - val_acc: 0.9851\n",
      "Epoch 192/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0543 - acc: 0.9852 - val_loss: 0.0545 - val_acc: 0.9851\n",
      "Epoch 193/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0543 - acc: 0.9852 - val_loss: 0.0545 - val_acc: 0.9851\n",
      "Epoch 194/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0543 - acc: 0.9852 - val_loss: 0.0545 - val_acc: 0.9851\n",
      "Epoch 195/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0542 - acc: 0.9852 - val_loss: 0.0544 - val_acc: 0.9851\n",
      "Epoch 196/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0542 - acc: 0.9852 - val_loss: 0.0544 - val_acc: 0.9851\n",
      "Epoch 197/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0542 - acc: 0.9852 - val_loss: 0.0544 - val_acc: 0.9851\n",
      "Epoch 198/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0542 - acc: 0.9852 - val_loss: 0.0544 - val_acc: 0.9851\n",
      "Epoch 199/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0542 - acc: 0.9852 - val_loss: 0.0544 - val_acc: 0.9851\n",
      "Epoch 200/200\n",
      "249646/249646 [==============================] - 1s 3us/step - loss: 0.0541 - acc: 0.9852 - val_loss: 0.0543 - val_acc: 0.9851\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAD8CAYAAABZ/vJZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wfFIslhzCi61AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h = model.fit(x_train, y_train, epochs=200, batch_size=1024, validation_data=(x_test, y_test))\n",
    "\n",
    "x_plt = np.arange(len(h.history['loss']))\n",
    "plt.plot(x_plt, h.history['loss'], label='train')\n",
    "plt.plot(x_plt, h.history['val_loss'], label='test')\n",
    "plt.show()\n",
    "plt.plot(x_plt, h.history['acc'], label='train')\n",
    "plt.plot(x_plt, h.history['val_acc'], label='test')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('det', 0.5102844, array([0.98608565, 0.01833067], dtype=float32))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist = 1\n",
    "pos1 = np.zeros(18)\n",
    "pos1[pos_2_1hot['DET']] = 1\n",
    "pos2 = np.zeros(18)\n",
    "pos2[pos_2_1hot['VERB']] = 1\n",
    "prediction = model.predict(np.concatenate(([dist], pos1, pos2)).reshape(1, -1))[0]\n",
    "i = np.argmax(prediction[2:])\n",
    "for l, j in labels_2_1hot.items():\n",
    "    if i == j:\n",
    "        label = l\n",
    "        break\n",
    "label, prediction[i+2], prediction[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('det', 0.45850724, array([0.11576077, 0.90845215], dtype=float32))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist = -1\n",
    "pos1 = np.zeros(18)\n",
    "pos1[pos_2_1hot['DET']] = 1\n",
    "pos2 = np.zeros(18)\n",
    "pos2[pos_2_1hot['VERB']] = 1\n",
    "prediction = model.predict(np.concatenate(([dist], pos1, pos2)).reshape(1, -1))[0]\n",
    "i = np.argmax(prediction[2:])\n",
    "for l, j in labels_2_1hot.items():\n",
    "    if i == j:\n",
    "        label = l\n",
    "        break\n",
    "label, prediction[i+2], prediction[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('amod', 0.46142182, array([0.96884716, 0.02792364], dtype=float32))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist = 1\n",
    "pos1 = np.zeros(18)\n",
    "pos1[pos_2_1hot['ADJ']] = 1\n",
    "pos2 = np.zeros(18)\n",
    "pos2[pos_2_1hot['NOUN']] = 1\n",
    "prediction = model.predict(np.concatenate(([dist], pos1, pos2)).reshape(1, -1))[0]\n",
    "i = np.argmax(prediction[2:])\n",
    "for l, j in labels_2_1hot.items():\n",
    "    if i == j:\n",
    "        label = l\n",
    "        break\n",
    "label, prediction[i+2], prediction[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('amod', 0.4423077, array([0.05432957, 0.9385203 ], dtype=float32))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist = -1\n",
    "pos1 = np.zeros(18)\n",
    "pos1[pos_2_1hot['ADJ']] = 1\n",
    "pos2 = np.zeros(18)\n",
    "pos2[pos_2_1hot['NOUN']] = 1\n",
    "prediction = model.predict(np.concatenate(([dist], pos1, pos2)).reshape(1, -1))[0]\n",
    "i = np.argmax(prediction[2:])\n",
    "for l, j in labels_2_1hot.items():\n",
    "    if i == j:\n",
    "        label = l\n",
    "        break\n",
    "label, prediction[i+2], prediction[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eisner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our model, we can use it to create the dependency parsing itself with Eisner's algorithm. \n",
    "\n",
    "[Explanatory video](https://youtu.be/du9VQaFEyeA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = np.array(sentences[3]).T\n",
    "pd.DataFrame(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eisner(sentence, model):\n",
    "    n = sentence.shape[0]\n",
    "\n",
    "    full_left = []\n",
    "    full_right = []\n",
    "    part_left = []\n",
    "    part_right = []\n",
    "    for i in range(n):\n",
    "        full_left.append([0])\n",
    "        full_right.append([0])\n",
    "        part_left.append([0])\n",
    "        part_right.append([0])\n",
    "\n",
    "    for m in range(1, n):\n",
    "        for i1 in range(n - m):\n",
    "            i2 = i1 + m\n",
    "\n",
    "            x, _ = create_example(sentence[i1], sentence[i2])\n",
    "            score = model.predict(x.reshape(1, -1))[0]\n",
    "            \n",
    "            score[0] = 0\n",
    "            score[1] = 0\n",
    "            if sentence[i1][governor_i] == sentence[i2][index_i]:\n",
    "                score[0] = 1\n",
    "            elif sentence[i2][governor_i] == sentence[i1][index_i]:\n",
    "                score[1] = 1\n",
    "\n",
    "            max_full = -1\n",
    "            max_q = -1\n",
    "            for q in range(i2 - i1):\n",
    "                q_line = q + i1\n",
    "                current = full_right[i1][q] + full_left[q_line + 1][i2 - q_line - 1]\n",
    "                if current > max_full:\n",
    "                    max_full = current\n",
    "                    max_q = q_line + 1\n",
    "            \n",
    "            part_left[i1].append(max_full + score[1])\n",
    "            part_right[i1].append(max_full + score[0])\n",
    "            \n",
    "            max_part_l = -1\n",
    "            max_part_r = -1\n",
    "            for q in range(i2 - i1):\n",
    "                q_line = q + i1\n",
    "                current_l = full_left[i1][q] + part_left[q_line][i2 - q_line]\n",
    "                current_r = part_right[i1][q + 1] + full_right[q_line + 1][i2 - q_line - 1]\n",
    "\n",
    "                if current_l > max_part_l:\n",
    "                    max_part_l = current_l\n",
    "                if current_r > max_part_r:\n",
    "                    max_part_r = current_r\n",
    "\n",
    "            full_left[i1].append(max_part_l)\n",
    "            full_right[i1].append(max_part_r)\n",
    "        print('-'*50)\n",
    "    return full_left, full_right, part_left, part_right \n",
    "\n",
    "full_left, full_right, part_left, part_right = eisner(sentence, model)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "1DGAN_v1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
